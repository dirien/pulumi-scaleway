// Code generated by pulumi-language-go DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package scaleway

import (
	"context"
	"reflect"

	"errors"
	"github.com/dirien/pulumi-scaleway/sdk/v2/go/scaleway/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// The InferenceModel resource allows you to upload and manage inference models in the Scaleway Inference ecosystem. Once registered, a model can be used in any InferenceDeployment resource.
//
// ## Example Usage
//
// ### Basic
//
// ```go
// package main
//
// import (
//
//	"github.com/dirien/pulumi-scaleway/sdk/v2/go/scaleway"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			_, err := scaleway.NewInferenceModel(ctx, "test", &scaleway.InferenceModelArgs{
//				Secret: pulumi.String("my-secret-token"),
//				Url:    pulumi.String("https://huggingface.co/agentica-org/DeepCoder-14B-Preview"),
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
//
// ### Deploy your own model on your managed inference
//
// ```go
// package main
//
// import (
//
//	"github.com/dirien/pulumi-scaleway/sdk/v2/go/scaleway"
//	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
//
// )
//
//	func main() {
//		pulumi.Run(func(ctx *pulumi.Context) error {
//			myModel, err := scaleway.NewInferenceModel(ctx, "myModel", &scaleway.InferenceModelArgs{
//				Url:    pulumi.String("https://huggingface.co/agentica-org/DeepCoder-14B-Preview"),
//				Secret: pulumi.String("my-secret-token"),
//			})
//			if err != nil {
//				return err
//			}
//			_, err = scaleway.NewInferenceDeployment(ctx, "myDeployment", &scaleway.InferenceDeploymentArgs{
//				NodeType: pulumi.String("H100"),
//				ModelId:  myModel.ID(),
//				PublicEndpoint: &scaleway.InferenceDeploymentPublicEndpointArgs{
//					IsEnabled: pulumi.Bool(true),
//				},
//				AcceptEula: pulumi.Bool(true),
//			})
//			if err != nil {
//				return err
//			}
//			return nil
//		})
//	}
//
// ```
//
// ## Import
//
// Models can be imported using, `{region}/{id}`, as shown below:
//
// bash
//
// ```sh
// $ pulumi import scaleway:index/inferenceModel:InferenceModel my_model fr-par/11111111-1111-1111-1111-111111111111
// ```
type InferenceModel struct {
	pulumi.CustomResourceState

	// The date and time of the creation of the model
	CreatedAt pulumi.StringOutput `pulumi:"createdAt"`
	// A textual description of the model (if available).
	Description pulumi.StringOutput `pulumi:"description"`
	// Whether the model requires end-user license agreement acceptance before use.
	HasEula pulumi.BoolOutput `pulumi:"hasEula"`
	// The name of the model. This must be unique within the project.
	Name pulumi.StringOutput `pulumi:"name"`
	// List of supported node types and their quantization options. Each entry contains:
	NodesSupports InferenceModelNodesSupportArrayOutput `pulumi:"nodesSupports"`
	// Size, in bits, of the model parameters.
	ParameterSizeBits pulumi.IntOutput `pulumi:"parameterSizeBits"`
	// `projectId`) The ID of the project the deployment is associated with.
	ProjectId pulumi.StringOutput `pulumi:"projectId"`
	// `region`) The region in which the deployment is created.
	Region pulumi.StringOutput `pulumi:"region"`
	// Authentication token used to pull the model from a private or gated URL (e.g., a Hugging Face access token with read permission).
	Secret pulumi.StringPtrOutput `pulumi:"secret"`
	// Total size, in bytes, of the model archive.
	SizeBytes pulumi.IntOutput `pulumi:"sizeBytes"`
	// The current status of the model (e.g., ready, error, etc.).
	Status pulumi.StringOutput `pulumi:"status"`
	// Tags associated with the model.
	Tags pulumi.StringArrayOutput `pulumi:"tags"`
	// The date and time of the last update of the model
	UpdatedAt pulumi.StringOutput `pulumi:"updatedAt"`
	// The HTTPS source URL from which the model will be downloaded. This is typically a Hugging Face repository URL (e.g., https://huggingface.co/agentica-org/DeepCoder-14B-Preview). The URL must be publicly accessible or require valid credentials via `secret`
	Url pulumi.StringOutput `pulumi:"url"`
}

// NewInferenceModel registers a new resource with the given unique name, arguments, and options.
func NewInferenceModel(ctx *pulumi.Context,
	name string, args *InferenceModelArgs, opts ...pulumi.ResourceOption) (*InferenceModel, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.Url == nil {
		return nil, errors.New("invalid value for required argument 'Url'")
	}
	if args.Secret != nil {
		args.Secret = pulumi.ToSecret(args.Secret).(pulumi.StringPtrInput)
	}
	secrets := pulumi.AdditionalSecretOutputs([]string{
		"secret",
	})
	opts = append(opts, secrets)
	opts = internal.PkgResourceDefaultOpts(opts)
	var resource InferenceModel
	err := ctx.RegisterResource("scaleway:index/inferenceModel:InferenceModel", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetInferenceModel gets an existing InferenceModel resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetInferenceModel(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *InferenceModelState, opts ...pulumi.ResourceOption) (*InferenceModel, error) {
	var resource InferenceModel
	err := ctx.ReadResource("scaleway:index/inferenceModel:InferenceModel", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering InferenceModel resources.
type inferenceModelState struct {
	// The date and time of the creation of the model
	CreatedAt *string `pulumi:"createdAt"`
	// A textual description of the model (if available).
	Description *string `pulumi:"description"`
	// Whether the model requires end-user license agreement acceptance before use.
	HasEula *bool `pulumi:"hasEula"`
	// The name of the model. This must be unique within the project.
	Name *string `pulumi:"name"`
	// List of supported node types and their quantization options. Each entry contains:
	NodesSupports []InferenceModelNodesSupport `pulumi:"nodesSupports"`
	// Size, in bits, of the model parameters.
	ParameterSizeBits *int `pulumi:"parameterSizeBits"`
	// `projectId`) The ID of the project the deployment is associated with.
	ProjectId *string `pulumi:"projectId"`
	// `region`) The region in which the deployment is created.
	Region *string `pulumi:"region"`
	// Authentication token used to pull the model from a private or gated URL (e.g., a Hugging Face access token with read permission).
	Secret *string `pulumi:"secret"`
	// Total size, in bytes, of the model archive.
	SizeBytes *int `pulumi:"sizeBytes"`
	// The current status of the model (e.g., ready, error, etc.).
	Status *string `pulumi:"status"`
	// Tags associated with the model.
	Tags []string `pulumi:"tags"`
	// The date and time of the last update of the model
	UpdatedAt *string `pulumi:"updatedAt"`
	// The HTTPS source URL from which the model will be downloaded. This is typically a Hugging Face repository URL (e.g., https://huggingface.co/agentica-org/DeepCoder-14B-Preview). The URL must be publicly accessible or require valid credentials via `secret`
	Url *string `pulumi:"url"`
}

type InferenceModelState struct {
	// The date and time of the creation of the model
	CreatedAt pulumi.StringPtrInput
	// A textual description of the model (if available).
	Description pulumi.StringPtrInput
	// Whether the model requires end-user license agreement acceptance before use.
	HasEula pulumi.BoolPtrInput
	// The name of the model. This must be unique within the project.
	Name pulumi.StringPtrInput
	// List of supported node types and their quantization options. Each entry contains:
	NodesSupports InferenceModelNodesSupportArrayInput
	// Size, in bits, of the model parameters.
	ParameterSizeBits pulumi.IntPtrInput
	// `projectId`) The ID of the project the deployment is associated with.
	ProjectId pulumi.StringPtrInput
	// `region`) The region in which the deployment is created.
	Region pulumi.StringPtrInput
	// Authentication token used to pull the model from a private or gated URL (e.g., a Hugging Face access token with read permission).
	Secret pulumi.StringPtrInput
	// Total size, in bytes, of the model archive.
	SizeBytes pulumi.IntPtrInput
	// The current status of the model (e.g., ready, error, etc.).
	Status pulumi.StringPtrInput
	// Tags associated with the model.
	Tags pulumi.StringArrayInput
	// The date and time of the last update of the model
	UpdatedAt pulumi.StringPtrInput
	// The HTTPS source URL from which the model will be downloaded. This is typically a Hugging Face repository URL (e.g., https://huggingface.co/agentica-org/DeepCoder-14B-Preview). The URL must be publicly accessible or require valid credentials via `secret`
	Url pulumi.StringPtrInput
}

func (InferenceModelState) ElementType() reflect.Type {
	return reflect.TypeOf((*inferenceModelState)(nil)).Elem()
}

type inferenceModelArgs struct {
	// The name of the model. This must be unique within the project.
	Name *string `pulumi:"name"`
	// `projectId`) The ID of the project the deployment is associated with.
	ProjectId *string `pulumi:"projectId"`
	// `region`) The region in which the deployment is created.
	Region *string `pulumi:"region"`
	// Authentication token used to pull the model from a private or gated URL (e.g., a Hugging Face access token with read permission).
	Secret *string `pulumi:"secret"`
	// The HTTPS source URL from which the model will be downloaded. This is typically a Hugging Face repository URL (e.g., https://huggingface.co/agentica-org/DeepCoder-14B-Preview). The URL must be publicly accessible or require valid credentials via `secret`
	Url string `pulumi:"url"`
}

// The set of arguments for constructing a InferenceModel resource.
type InferenceModelArgs struct {
	// The name of the model. This must be unique within the project.
	Name pulumi.StringPtrInput
	// `projectId`) The ID of the project the deployment is associated with.
	ProjectId pulumi.StringPtrInput
	// `region`) The region in which the deployment is created.
	Region pulumi.StringPtrInput
	// Authentication token used to pull the model from a private or gated URL (e.g., a Hugging Face access token with read permission).
	Secret pulumi.StringPtrInput
	// The HTTPS source URL from which the model will be downloaded. This is typically a Hugging Face repository URL (e.g., https://huggingface.co/agentica-org/DeepCoder-14B-Preview). The URL must be publicly accessible or require valid credentials via `secret`
	Url pulumi.StringInput
}

func (InferenceModelArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*inferenceModelArgs)(nil)).Elem()
}

type InferenceModelInput interface {
	pulumi.Input

	ToInferenceModelOutput() InferenceModelOutput
	ToInferenceModelOutputWithContext(ctx context.Context) InferenceModelOutput
}

func (*InferenceModel) ElementType() reflect.Type {
	return reflect.TypeOf((**InferenceModel)(nil)).Elem()
}

func (i *InferenceModel) ToInferenceModelOutput() InferenceModelOutput {
	return i.ToInferenceModelOutputWithContext(context.Background())
}

func (i *InferenceModel) ToInferenceModelOutputWithContext(ctx context.Context) InferenceModelOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InferenceModelOutput)
}

// InferenceModelArrayInput is an input type that accepts InferenceModelArray and InferenceModelArrayOutput values.
// You can construct a concrete instance of `InferenceModelArrayInput` via:
//
//	InferenceModelArray{ InferenceModelArgs{...} }
type InferenceModelArrayInput interface {
	pulumi.Input

	ToInferenceModelArrayOutput() InferenceModelArrayOutput
	ToInferenceModelArrayOutputWithContext(context.Context) InferenceModelArrayOutput
}

type InferenceModelArray []InferenceModelInput

func (InferenceModelArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*InferenceModel)(nil)).Elem()
}

func (i InferenceModelArray) ToInferenceModelArrayOutput() InferenceModelArrayOutput {
	return i.ToInferenceModelArrayOutputWithContext(context.Background())
}

func (i InferenceModelArray) ToInferenceModelArrayOutputWithContext(ctx context.Context) InferenceModelArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InferenceModelArrayOutput)
}

// InferenceModelMapInput is an input type that accepts InferenceModelMap and InferenceModelMapOutput values.
// You can construct a concrete instance of `InferenceModelMapInput` via:
//
//	InferenceModelMap{ "key": InferenceModelArgs{...} }
type InferenceModelMapInput interface {
	pulumi.Input

	ToInferenceModelMapOutput() InferenceModelMapOutput
	ToInferenceModelMapOutputWithContext(context.Context) InferenceModelMapOutput
}

type InferenceModelMap map[string]InferenceModelInput

func (InferenceModelMap) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*InferenceModel)(nil)).Elem()
}

func (i InferenceModelMap) ToInferenceModelMapOutput() InferenceModelMapOutput {
	return i.ToInferenceModelMapOutputWithContext(context.Background())
}

func (i InferenceModelMap) ToInferenceModelMapOutputWithContext(ctx context.Context) InferenceModelMapOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InferenceModelMapOutput)
}

type InferenceModelOutput struct{ *pulumi.OutputState }

func (InferenceModelOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**InferenceModel)(nil)).Elem()
}

func (o InferenceModelOutput) ToInferenceModelOutput() InferenceModelOutput {
	return o
}

func (o InferenceModelOutput) ToInferenceModelOutputWithContext(ctx context.Context) InferenceModelOutput {
	return o
}

// The date and time of the creation of the model
func (o InferenceModelOutput) CreatedAt() pulumi.StringOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringOutput { return v.CreatedAt }).(pulumi.StringOutput)
}

// A textual description of the model (if available).
func (o InferenceModelOutput) Description() pulumi.StringOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringOutput { return v.Description }).(pulumi.StringOutput)
}

// Whether the model requires end-user license agreement acceptance before use.
func (o InferenceModelOutput) HasEula() pulumi.BoolOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.BoolOutput { return v.HasEula }).(pulumi.BoolOutput)
}

// The name of the model. This must be unique within the project.
func (o InferenceModelOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringOutput { return v.Name }).(pulumi.StringOutput)
}

// List of supported node types and their quantization options. Each entry contains:
func (o InferenceModelOutput) NodesSupports() InferenceModelNodesSupportArrayOutput {
	return o.ApplyT(func(v *InferenceModel) InferenceModelNodesSupportArrayOutput { return v.NodesSupports }).(InferenceModelNodesSupportArrayOutput)
}

// Size, in bits, of the model parameters.
func (o InferenceModelOutput) ParameterSizeBits() pulumi.IntOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.IntOutput { return v.ParameterSizeBits }).(pulumi.IntOutput)
}

// `projectId`) The ID of the project the deployment is associated with.
func (o InferenceModelOutput) ProjectId() pulumi.StringOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringOutput { return v.ProjectId }).(pulumi.StringOutput)
}

// `region`) The region in which the deployment is created.
func (o InferenceModelOutput) Region() pulumi.StringOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringOutput { return v.Region }).(pulumi.StringOutput)
}

// Authentication token used to pull the model from a private or gated URL (e.g., a Hugging Face access token with read permission).
func (o InferenceModelOutput) Secret() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringPtrOutput { return v.Secret }).(pulumi.StringPtrOutput)
}

// Total size, in bytes, of the model archive.
func (o InferenceModelOutput) SizeBytes() pulumi.IntOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.IntOutput { return v.SizeBytes }).(pulumi.IntOutput)
}

// The current status of the model (e.g., ready, error, etc.).
func (o InferenceModelOutput) Status() pulumi.StringOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringOutput { return v.Status }).(pulumi.StringOutput)
}

// Tags associated with the model.
func (o InferenceModelOutput) Tags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringArrayOutput { return v.Tags }).(pulumi.StringArrayOutput)
}

// The date and time of the last update of the model
func (o InferenceModelOutput) UpdatedAt() pulumi.StringOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringOutput { return v.UpdatedAt }).(pulumi.StringOutput)
}

// The HTTPS source URL from which the model will be downloaded. This is typically a Hugging Face repository URL (e.g., https://huggingface.co/agentica-org/DeepCoder-14B-Preview). The URL must be publicly accessible or require valid credentials via `secret`
func (o InferenceModelOutput) Url() pulumi.StringOutput {
	return o.ApplyT(func(v *InferenceModel) pulumi.StringOutput { return v.Url }).(pulumi.StringOutput)
}

type InferenceModelArrayOutput struct{ *pulumi.OutputState }

func (InferenceModelArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]*InferenceModel)(nil)).Elem()
}

func (o InferenceModelArrayOutput) ToInferenceModelArrayOutput() InferenceModelArrayOutput {
	return o
}

func (o InferenceModelArrayOutput) ToInferenceModelArrayOutputWithContext(ctx context.Context) InferenceModelArrayOutput {
	return o
}

func (o InferenceModelArrayOutput) Index(i pulumi.IntInput) InferenceModelOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) *InferenceModel {
		return vs[0].([]*InferenceModel)[vs[1].(int)]
	}).(InferenceModelOutput)
}

type InferenceModelMapOutput struct{ *pulumi.OutputState }

func (InferenceModelMapOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*map[string]*InferenceModel)(nil)).Elem()
}

func (o InferenceModelMapOutput) ToInferenceModelMapOutput() InferenceModelMapOutput {
	return o
}

func (o InferenceModelMapOutput) ToInferenceModelMapOutputWithContext(ctx context.Context) InferenceModelMapOutput {
	return o
}

func (o InferenceModelMapOutput) MapIndex(k pulumi.StringInput) InferenceModelOutput {
	return pulumi.All(o, k).ApplyT(func(vs []interface{}) *InferenceModel {
		return vs[0].(map[string]*InferenceModel)[vs[1].(string)]
	}).(InferenceModelOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*InferenceModelInput)(nil)).Elem(), &InferenceModel{})
	pulumi.RegisterInputType(reflect.TypeOf((*InferenceModelArrayInput)(nil)).Elem(), InferenceModelArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*InferenceModelMapInput)(nil)).Elem(), InferenceModelMap{})
	pulumi.RegisterOutputType(InferenceModelOutput{})
	pulumi.RegisterOutputType(InferenceModelArrayOutput{})
	pulumi.RegisterOutputType(InferenceModelMapOutput{})
}
